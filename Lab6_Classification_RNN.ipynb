{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v42xWF71CGSp",
        "yD3COp6YECo4",
        "hcx0QqRgIUrq",
        "NzFm6PGJI1So",
        "LxEk6_qLKnZD",
        "96r_MDRgN6eS",
        "caV4lvLXOMsY",
        "C8oTHpbxOcoA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Crearemos una red neuronal recurrente para clasificar reviews de películas. Las clases son \"positivo\" o \"negativo\".\n",
        "Primero descargamos los datos"
      ],
      "metadata": {
        "id": "l3HU_r_9BL2d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLr8whGj-wCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13648d4-b10b-4b26-aae2-c4fcc42f301c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-12 00:24:19--  https://www.ivan-sipiran.com/downloads/labels.txt\n",
            "Resolving www.ivan-sipiran.com (www.ivan-sipiran.com)... 66.96.149.31\n",
            "Connecting to www.ivan-sipiran.com (www.ivan-sipiran.com)|66.96.149.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225000 (220K) [text/plain]\n",
            "Saving to: ‘labels.txt’\n",
            "\n",
            "labels.txt          100%[===================>] 219.73K   978KB/s    in 0.2s    \n",
            "\n",
            "2025-09-12 00:24:20 (978 KB/s) - ‘labels.txt’ saved [225000/225000]\n",
            "\n",
            "--2025-09-12 00:24:20--  https://www.ivan-sipiran.com/downloads/reviews.txt\n",
            "Resolving www.ivan-sipiran.com (www.ivan-sipiran.com)... 66.96.149.31\n",
            "Connecting to www.ivan-sipiran.com (www.ivan-sipiran.com)|66.96.149.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33678267 (32M) [text/plain]\n",
            "Saving to: ‘reviews.txt’\n",
            "\n",
            "reviews.txt         100%[===================>]  32.12M  25.0MB/s    in 1.3s    \n",
            "\n",
            "2025-09-12 00:24:22 (25.0 MB/s) - ‘reviews.txt’ saved [33678267/33678267]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget -c https://www.ivan-sipiran.com/downloads/labels.txt\n",
        "!wget -c https://www.ivan-sipiran.com/downloads/reviews.txt\n",
        "!mv *.txt data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargamos los archivos de texto\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "  reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "  labels = f.read()\n",
        "\n",
        "#reviews y labels como un solo string que almacena todo el archivo"
      ],
      "metadata": {
        "id": "nvzT3lz-BbRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews[:200])\n",
        "print()\n",
        "print(labels[:18])"
      ],
      "metadata": {
        "id": "smFhI8QQB0-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edadf86-5dec-48df-acc5-397470db2ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  \n",
            "\n",
            "positive\n",
            "negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-procesamiento de datos\n",
        "\n",
        "Necesitamos tener los datos en una forma que haga posible el uso de una red nueronal para procesarlo. En este laboratorio, usaremos una representación a nivel de palabras, por lo que necesitamos una forma de representar cada palabra de forma numérica. Aquí aprenderemos cómo procesar el texto y convertirlo en una representación idónea.\n",
        "\n",
        "Para simplificar el trabajo, tendremos en cuenta:\n",
        "\n",
        "*   No nos interesan los signos de puntuación\n",
        "*   Dividir los reviews (un solo string) en strings individuales para cada review. Usaremos el `\\n` como delimitador.\n",
        "\n",
        "Primero, eliminamos los signos de puntuación y dividimos el texto en palabras indiduales\n",
        "\n"
      ],
      "metadata": {
        "id": "v42xWF71CGSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "reviews = reviews.lower()\n",
        "\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
        "\n",
        "#all_text[:500]"
      ],
      "metadata": {
        "id": "87bm9QOaCDmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f36b164-4d26-414a-f11f-f5608bcbcd8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removemos los saltos de línea, y juntamos todo el texto de nuevo\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ''.join(reviews_split)\n",
        "\n",
        "words = all_text.split()\n",
        "print(words[:30])"
      ],
      "metadata": {
        "id": "dTOPXGHkDWVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d661390-6be6-46d6-afb1-6b307fb4e0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codificando las palabras\n",
        "\n",
        "Vamos a representar cada palabra con un único número entero que representará su índice. Para eso construiremos un diccionario que mapee palabras a números."
      ],
      "metadata": {
        "id": "yD3COp6YECo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(words) #Construye un diccionario de palabras. Las claves son las palabras y los valores son la frecuencia\n",
        "vocab = sorted(counts, key=counts.get, reverse=True) #Ordenamos la palabras por frecuencia\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} #Construimos un diccionario para mapear palabra a número entero. Empezamos los índices en 1\n",
        "\n",
        "#Ahora convertimos cada palabra de los reviews en índices\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n"
      ],
      "metadata": {
        "id": "fUNyXic8DmyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cada review ahora se representa como una secuencia de números (índices)\n",
        "print(reviews_split[0])\n",
        "print(reviews_ints[0])"
      ],
      "metadata": {
        "id": "P0pND9oiE2ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42eeab68-59c7-4d5d-d48c-9e0c0fcebe2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   \n",
            "[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cuántas palabras hay en el diccionario?\n",
        "print('Palabras únicas:', len(vocab_to_int))\n",
        "print()"
      ],
      "metadata": {
        "id": "r3wNZ7YaE32Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a25284-472f-4b7d-929a-acd6d99a3523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras únicas: 74072\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding de etiquetas\n",
        "Las etiquetas se convierten a números para poder ser ingresadas a la red neuronal.\n"
      ],
      "metadata": {
        "id": "hcx0QqRgIUrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split]) # 1 si es positivo, 0 si no"
      ],
      "metadata": {
        "id": "ALMjVSMxISGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Longitud de Secuencias\n",
        "Los reviews tienen distinto tamaño. Para poder procesar estos datos de manera efectiva en una red neuronal, tenemos que uniformizar los tamaños. Esto permite la creación de batches para el entrenamiento."
      ],
      "metadata": {
        "id": "NzFm6PGJI1So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sacamos algunas estadísticas de los datos\n",
        "review_lens = Counter([len(x) for x in reviews_ints]) #Contamos cuantas palabras hay en cada review\n",
        "print(\"Reviews de longitud cero:\", review_lens[0])\n",
        "print('Máxima longitud:', max(review_lens))"
      ],
      "metadata": {
        "id": "LuZ-S1JwIsQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05379170-674c-4f1e-c38f-02f9963c5e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews de longitud cero: 1\n",
            "Máxima longitud: 2514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay reviews vacíos y reviews muy grandes para ser procesados por una RNN. Primero eliminamos los reviews vacíos y truncamos los reviews muy grandes"
      ],
      "metadata": {
        "id": "EbtpqglKJppO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reviews antes de eliminación:', len(reviews_ints))\n",
        "\n",
        "#Extraemos los índices de todos los reviews que tienen longitud > 0\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review)!=0]\n",
        "\n",
        "#Nos quedamos solo con los reviews con longitud > 0\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "\n",
        "#Lo mismo con los labels\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Reviews después de eliminación:', len(reviews_ints))"
      ],
      "metadata": {
        "id": "LKf0xi3FJTpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b3b36bc-fcf6-49a7-ca1e-1c60ed9ee898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews antes de eliminación: 25001\n",
            "Reviews después de eliminación: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "La estrategia para uniformizar el tamaño de las secuencias es como sigue:\n",
        "\n",
        "\n",
        "*   Definir un tamaño para las secuencias: `seq_length`\n",
        "*   Reviews con más de `seq_length` palabras, truncamos el review a las primeras `seq_length` palabras.\n",
        "*   Reviews con menos de `seq_length` palabras, aplicamos padding con 0's al inicio de la secuencia. Por ejemplo el review `['best', 'movie', 'ever']`, `[117, 18, 128]` quedaría como `[0, 0 ,0, ...,117, 18, 128]`\n",
        "\n",
        "Como cada review ahora tendrá la misma longitud podemos convertir los datos en un array 2D, más conveniente para el proceso posterior."
      ],
      "metadata": {
        "id": "LxEk6_qLKnZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "  features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "  #Para cada review, se coloca en la matriz\n",
        "  for i, row in enumerate(reviews_ints):\n",
        "    features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "EurK6LTiKlNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Probamos el padding\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "print(features.shape)\n",
        "print(features[:30,:30])"
      ],
      "metadata": {
        "id": "Y4mHBmSvM60I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6781c82-47f8-4817-9acf-b47e699cb289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 200)\n",
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35  1819    16\n",
            "    154    19   114     3  1305     5   336   147    22     1   857    12\n",
            "     70   281  1168   399    36   120]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819    56    17\n",
            "   4504  5616   140 11725     5   996  4919  2933  4462   566  1201    36\n",
            "      6  1518    96     3   744     4]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5     1   730\n",
            "      5    66  8057     8    14    30     4   109    99    10   293    17\n",
            "     60   798    19    11    14     1]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0    11   215    23     1  1686  2069  1565   867\n",
            "      6     8     1  2765  2116  2069]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325    73   154\n",
            "      5    94     3  3875    20     1    84   111  2613 46423    13   551\n",
            "   3951    62    14    91     9  7076]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673  4608  2069\n",
            "   1565     3  6452   533     2   512   307   723   121  3149   455  7692\n",
            "     34     1  7475    63     4     3]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628  8179   865\n",
            "  10884  4315   436   297 37453  2615 24112 11403  1816  3014 10173  2266\n",
            "  21034     7     7 14051 32312   403]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182   309     9\n",
            "      1  1994  4395    17    31   132   976  1258    25    47   220    11\n",
            "   2022    17   375     1  1375     4]\n",
            " [    1    20     6    76    40     6    58    81    95     5   133     8\n",
            "      1    63     6   799     2  5789   257    16    11   657    10    89\n",
            "     23   102    90    78   122    48]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614     9    34\n",
            "      1   399   448  4276    34  4608    58    30     2   147    25  2221\n",
            "     44     2  1367    12   111   301]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6  1231   407\n",
            "      9    27   203    76   860     2    27   917     6    44    15 11408\n",
            "     15     8    51    29   205    27]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125    62    28\n",
            "     77   344   110     9   662   209  1796 24115    42 26232  1603     9\n",
            "      1  5402     4 46431    28   810]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30     4     1\n",
            "    128    99     4     1  1454     7     7  2069  1565   403    30     4\n",
            "     27   117   351     9   190     1]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0    11    18   206    29\n",
            "      9   341    16     1    90   354]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Partición de los datos\n"
      ],
      "metadata": {
        "id": "96r_MDRgN6eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:] # Reviews\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:] # Etiquetas\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:] # 50% test\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:] # 50% validación\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeatures:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "metadata": {
        "id": "jvdP-cDqNNWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0f9684-c795-4357-e727-007928efac51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\tFeatures:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets y Dataloaders\n"
      ],
      "metadata": {
        "id": "caV4lvLXOMsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# crear Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "print(\"N° interaciones train: \", len(train_loader))\n",
        "print(\"N° interaciones validación: \", len(valid_loader))\n",
        "print(\"N° interaciones test: \", len(test_loader))"
      ],
      "metadata": {
        "id": "izkA1cS1OHHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ccc607-4112-43a6-9435-3b69e299092d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N° interaciones train:  400\n",
            "N° interaciones validación:  50\n",
            "N° interaciones test:  50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creación de red neuronal\n",
        "Nuestra red tendrá los siguientes componentes:\n",
        "1. Una [capa embedding](https://pytorch.org/docs/stable/nn.html#embedding) que convierta tokens de palabras (índices) en embeddings de un tamaño específico.\n",
        "2. Una [capa LSTM](https://pytorch.org/docs/stable/nn.html#lstm) definida por el tamaño del estado oculto y el número de capas.\n",
        "3. Una capa de salida fully-connected que mapee la salida del LSTM al tamaño de salida deseado\n",
        "4. Una capa de activación sigmoide que convierta la salida a valores 0-1; debe retornar **solo la última salida sigmoide** como salida d ela red.\n",
        "\n",
        "### La capa embedding\n",
        "\n",
        "Necesitamos agregar una [capa embedding](https://pytorch.org/docs/stable/nn.html#embedding). Una opción podría ser usar one-hot encoding para el vocabulario, pero tenemos demasiadas palabras (~74,000). Así que mejor usamos una capa de embedding que sirva como tabla look-up para una determinada palabra. También se podría usar Word2Vec (embeddings preentrenados), pero ahora usamos una capa embedding que sea aprendida para este problema específico, usándolo como una forma de reducción de la dimensión del vocabulario original.\n",
        "\n",
        "### La capa LSTM\n",
        "\n",
        "Crearemos un [LSTM](https://pytorch.org/docs/stable/nn.html#lstm) como red recurrente, con los siguientes parámetros input_size, hidden_dim, el número de capas, la probabilidad de dropout (para dropout entre múltiples), y un parámetro batch_first.\n",
        "\n"
      ],
      "metadata": {
        "id": "C8oTHpbxOcoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chequear si tenemos GPU\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "metadata": {
        "id": "adKz6u-SOXuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bf0b28-f4c0-471b-8542-214e197943d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos la red neuronal\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Capas embedding y LSTM\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(drop_prob) # Apaga aleatoriamente 50% de las neuronas\n",
        "\n",
        "        # Capa lineal y sigmoide\n",
        "        self.fc = nn.Linear(hidden_dim, output_size) # 256, 1\n",
        "        self.sig = nn.Sigmoid() # Activación. Toma un número y lo convierte entre 0 y 1\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        #Tomamos solo el último valor de salida del LSTM\n",
        "        lstm_out = lstm_out[:,-1,:]\n",
        "\n",
        "        # dropout y fully-connected\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # sigmoide\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "        # retornar sigmoide y último estado oculto\n",
        "        return sig_out, hidden\n",
        "\n",
        "    # crea/reinicia el estado inicial del LSTM (oculto y memoria), con tamaño (n_layers, batch_size, hidden_dim)\n",
        "    # lleno de ceros\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Crea dos nuevos tensores con tamaño n_layers x batch_size x hidden_dim,\n",
        "        # inicializados a cero, para estado oculto y memoria de LSTM\n",
        "        weight = next(self.parameters()).data # Obtiene el tensor de pesos de la red\n",
        "\n",
        "        if(train_on_gpu):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "T3amSRw7QSHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciamos la red\n",
        "vocab_size = len(vocab_to_int) + 1 # +1 for zero padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400 # Tamaño del vector de embedding. Arbitrario, no es fijo, depende del problema. 25000 x 400\n",
        "hidden_dim = 256 # Estado oculto del LSTM. En cada paso del tiempo, el LSTM porduce un vector con 256 valores.\n",
        "                 # Representa lo que la red ha recordado hasta ese punto de la secuencia.\n",
        "n_layers = 2 # 2 LSTM\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "id": "1IV3WtMJTYi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863065b7-3a60-4cab-edc2-3d904ec0fb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento\n",
        "\n",
        "Usaremos Binary Cross Entropy Loss, ideal para problemas binarios que usan sigmoide. Usamos gradient clipping también para evitar que los gradientes crezcan mucho."
      ],
      "metadata": {
        "id": "QG74D0RDfLNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "m-U2nRyYT2RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sobre [Exploiding gradients](https://www.geeksforgeeks.org/deep-learning/vanishing-and-exploding-gradients-problems-in-deep-learning/)"
      ],
      "metadata": {
        "id": "EGMeH4i1AHZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training params\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping: evita el \"Exploiding Gradient\"\n",
        "\n",
        "# Enviar red al GPU\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# Bucle de entrenamiento\n",
        "for e in range(epochs):\n",
        "    # Inicializar estado oculto\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # Bucle para batchs\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Crear nuevas variables para estados ocultos, sino se haría\n",
        "        # backprop para todos los pasos del bucle\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        net.zero_grad()\n",
        "\n",
        "        # Hacer pasada forward\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # Calcular loss y hacer backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # gradient clipping\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Mensajes\n",
        "        if counter % print_every == 0:\n",
        "            # Validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Época: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Paso: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "ootGfDVmdR0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3dec7d1-aac5-4f53-d5d3-2775ab6672a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 1/4... Paso: 100... Loss: 0.558549... Val Loss: 0.623495\n",
            "Época: 1/4... Paso: 200... Loss: 0.706370... Val Loss: 0.691486\n",
            "Época: 1/4... Paso: 300... Loss: 0.605062... Val Loss: 0.631216\n",
            "Época: 1/4... Paso: 400... Loss: 0.603836... Val Loss: 0.678208\n",
            "Época: 2/4... Paso: 500... Loss: 0.531476... Val Loss: 0.567647\n",
            "Época: 2/4... Paso: 600... Loss: 0.608703... Val Loss: 0.583971\n",
            "Época: 2/4... Paso: 700... Loss: 0.535602... Val Loss: 0.542013\n",
            "Época: 2/4... Paso: 800... Loss: 0.422123... Val Loss: 0.578623\n",
            "Época: 3/4... Paso: 900... Loss: 0.502173... Val Loss: 0.477444\n",
            "Época: 3/4... Paso: 1000... Loss: 0.346903... Val Loss: 0.498183\n",
            "Época: 3/4... Paso: 1100... Loss: 0.214732... Val Loss: 0.449111\n",
            "Época: 3/4... Paso: 1200... Loss: 0.305798... Val Loss: 0.430269\n",
            "Época: 4/4... Paso: 1300... Loss: 0.262447... Val Loss: 0.524449\n",
            "Época: 4/4... Paso: 1400... Loss: 0.201503... Val Loss: 0.506572\n",
            "Época: 4/4... Paso: 1500... Loss: 0.109804... Val Loss: 0.464978\n",
            "Época: 4/4... Paso: 1600... Loss: 0.159581... Val Loss: 0.477936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "Probamos de dos formas:\n",
        "\n",
        "\n",
        "*   Calcular accuracy de test\n",
        "*   Probar inferencia con reviews nuevos\n",
        "\n"
      ],
      "metadata": {
        "id": "x2uMK73bgIm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular accuracy de test\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# Iniciar estado oculto\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    output, h = net(inputs, h)\n",
        "\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # Convertir probabilidades a clases (0,1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    # Comparar predicciones a labels\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# Accuracy de test\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "3uokiuQ5dZnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f111a78b-36d2-4ff7-f4f3-de37c32be556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.451\n",
            "Test accuracy: 0.819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencia sobre reviews"
      ],
      "metadata": {
        "id": "o8Fd8Or6g2Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower()\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    test_words = test_text.split()\n",
        "\n",
        "    test_ints = []\n",
        "\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "\n",
        "    return test_ints"
      ],
      "metadata": {
        "id": "I6IRHh3deeWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    test_ints = tokenize_review(test_review)\n",
        "\n",
        "    seq_length = sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "\n",
        "    batch_size = feature_tensor.size(0)\n",
        "\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    if(train_on_gpu):\n",
        "      feature_tensor = feature_tensor.cuda()\n",
        "\n",
        "    # Aqui ocurre la predicción\n",
        "    output, h = net(feature_tensor, h)\n",
        "\n",
        "    pred = torch.round(output.squeeze())\n",
        "    print('Valor predicho, antes del redondeo: {:.6f}'.format(output.item()))\n",
        "\n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    if(pred.item()==1):\n",
        "      print('Review positivo!')\n",
        "    else:\n",
        "      print('Review negativo!')"
      ],
      "metadata": {
        "id": "u5Jh6kEZehnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# negative test review\n",
        "test_review_neg = 'Honestly, the movie sucks.'\n",
        "\n",
        "# positive test review\n",
        "test_review_pos = 'Well done. This is the best movie I have ever watched so far.'"
      ],
      "metadata": {
        "id": "Qok9IMfGekyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=200\n",
        "predict(net, test_review_neg, seq_length)\n",
        "predict(net, test_review_pos, seq_length)"
      ],
      "metadata": {
        "id": "BOWZ-sRcenf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fe7c06-fd07-4a72-a851-260d80e2bd40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valor predicho, antes del redondeo: 0.049095\n",
            "Review negativo!\n",
            "Valor predicho, antes del redondeo: 0.959382\n",
            "Review positivo!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRV3pv6PepcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}