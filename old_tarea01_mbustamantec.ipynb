{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Nombre\n",
        "*   Marco Antonio Bustamante Calderón"
      ],
      "metadata": {
        "id": "AFRRvi1rx0AF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarea 1 Deep Learning: Redes Convolucionales\n",
        "En esta tarea vas a experimentar con redes convolucionales y su comparación con redes MLP. Lo primero que haremos es importar los paquetes importantes para nuestro notebook e inicializar el gpu. Asegúrate que tu notebook se ejecuta sobre un GPU. Si es así, al ejecutar la siguiente celda debería salirte el mensaje \"cuda\"."
      ],
      "metadata": {
        "id": "4HnJ1oS2x0-Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z0hQq5G3f24u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f026a40-5877-4f75-bae7-a00702584712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación hay un conjunto de funciones que puedes usar para hacer tus experimentos. Los detalles de cada función están como comentario antes del código de la función."
      ],
      "metadata": {
        "id": "n_1eTebOxyVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta función permite inicializar todas las semillas de números pseudoaleatorios.\n",
        "# Puedes usar esta función para resetear los generadores de números aleatorios\n",
        "def iniciar_semillas():\n",
        "  SEED = 1234\n",
        "\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "#Función para computar el accuracy. Se asume que predicciones y etiquetas son tensores en el GPU\n",
        "def calculate_accuracy(y_pred, y):\n",
        "  top_pred = y_pred.argmax(1, keepdim=True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "  acc = correct.float()/y.shape[0]\n",
        "  return acc\n",
        "\n",
        "#Función para entrenar una época de un modelo. Recibe como parámetros\n",
        "#     -model: una red neuronal\n",
        "#     -iterator: un iterador de la data a usar para el entrenamiento (generalmente creado con un DataLoader)\n",
        "#     -optimizer: el optimizador para el entrenamiento\n",
        "#     -criterion: la función de loss\n",
        "#     -device: dispositivo a usar para el entrenamiento\n",
        "#\n",
        "#Devuelve el loss promedio y el accuracy promedio de la época (promedio de todos los batches)\n",
        "def train_one_epoch(model, iterator, optimizer, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  #We have to set the neural network in training mode. This is because during\n",
        "  #training, we need gradients and complementary data to ease the computation\n",
        "  model.train()\n",
        "\n",
        "  #Training loop\n",
        "  for (x, y) in iterator:\n",
        "    x = x.to(device) #Data\n",
        "    y = y.long().to(device) #Labels\n",
        "\n",
        "    optimizer.zero_grad() #Clean gradients\n",
        "\n",
        "    y_pred = model(x) #Feed the network with data\n",
        "\n",
        "    loss = criterion(y_pred, y) #Compute the loss\n",
        "\n",
        "    acc = calculate_accuracy(y_pred, y) #Compute the accuracy\n",
        "\n",
        "    loss.backward() #Compute gradients\n",
        "\n",
        "    optimizer.step() #Apply update rules\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "#Función que evalúa una red neuronal con un conjunto de datos de prueba. Recibe como parámetros\n",
        "#     -model: una red neuronal\n",
        "#     -iterator: un iterador de la data a usar para el entrenamiento (generalmente creado con un DataLoader)\n",
        "#     -criterion: la función de loss\n",
        "#     -device: dispositivo a usar para el entrenamiento\n",
        "#Devuelve el loss promedio y el accuracy promedio de la época (promedio de todos los batches)\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  #We put the network in testing mode\n",
        "  #In this mode, Pytorch doesn't use features only reserved for\n",
        "  #training (dropout for instance)\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad(): #disable the autograd engine (save computation and memory)\n",
        "\n",
        "    for (x, y) in iterator:\n",
        "      x = x.to(device)\n",
        "      y = y.long().to(device)\n",
        "\n",
        "      y_pred= model(x)\n",
        "\n",
        "      loss = criterion(y_pred, y)\n",
        "\n",
        "      acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "#Calcula el tiempo transcurrido entre dos timestamps\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "#Esta función realiza el entrenamiento completo de una red. Recibe como parámetros:\n",
        "#     -network: la red neuronal\n",
        "#     -optimizer: el optimizador para entrenamiento\n",
        "#     -train_loader: el dataloader de datos de entrenamiento\n",
        "#     -tes_loader: el dataloader de datos de prueba\n",
        "#     -name: nombre a usar para guardar en disco la red con el mejor accuracy\n",
        "\n",
        "def train_complete(network, optimizer, train_loader, test_loader, name):\n",
        "\n",
        "  #Se envían la red y la función de loss al GPU\n",
        "  network = network.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  #Fijar el entrenamiento en 10 épocas siempre\n",
        "  EPOCHS = 10\n",
        "\n",
        "  best_valid_acc = float('-inf')\n",
        "  n_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
        "  print(f'The model has {n_params:,} trainable parameters')\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    #Train + validation cycles\n",
        "    train_loss, train_acc = train_one_epoch(network, train_loader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(network, test_loader, criterion, device)\n",
        "\n",
        "    #Si encontramos un modelo con accuracy de validación mayor, lo guardamos\n",
        "    if valid_acc > best_valid_acc:\n",
        "     best_valid_acc = valid_acc\n",
        "     torch.save(network.state_dict(), f'{name}.pt')\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "  #Cuando se termina el entrenamiento, cargamos el mejor modelo guardado y calculamos el accuracy de prueba\n",
        "  network.load_state_dict(torch.load(f'{name}.pt'))\n",
        "\n",
        "  test_loss , test_acc = evaluate(network, test_loader, criterion, device)\n",
        "  print(f'Test Loss: {test_loss:.3f} | Mejor test acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "BTwkQ3KguZSw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para nuestros experimentos usaremos el dataset CIFAR10, que consta de imágenes RGB de 32x32 píxeles que representan distintos objetos y animales clasificados en 10 clases. Inicializamos los hiperparámetros y cargamos los conjuntos de datos de entrenamiento y prueba. También se crean los dataloaders correspondientes."
      ],
      "metadata": {
        "id": "aeBLMf8tutop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(test_loader) #Crea un iterador sobre la lista de batches\n",
        "\n",
        "batchId, (exampleData, exampleTargets) = next(examples) #Obtenemos el siguiente batch\n",
        "print('Numero de batch: {}'.format(batchId))\n",
        "print('Batch Shape', exampleData.shape)\n",
        "print('Target Shape', exampleTargets.shape)\n",
        "\n",
        "#Mostramos los primeros 16 elementos del batch con su etiqueta\n",
        "plt.figure()\n",
        "for i in range(16):\n",
        "  plt.subplot(4, 4, i+1)\n",
        "  plt.tight_layout()\n",
        "  img = exampleData[i] / 2 + 0.5 # unnormalize\n",
        "  img = img.permute(1, 2, 0)\n",
        "  plt.imshow(img, interpolation='none')\n",
        "  plt.title(class_names[exampleTargets[i]])\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "metadata": {
        "id": "fGjeTESCvSnD",
        "outputId": "718c321f-2ecc-4107-8258-e607dc95fb58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1517601314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Crea un iterador sobre la lista de batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatchId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexampleData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexampleTargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Obtenemos el siguiente batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Numero de batch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch Shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexampleData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "#mab: Se modifican hiperparámetros\n",
        "input_size = 3*32*32\n",
        "#input_size = 784\n",
        "num_classes = 10\n",
        "batch_size_train = 1000\n",
        "batch_size_test = 1000\n",
        "momentum = 0.9\n",
        "log_interval = 100\n",
        "learning_rate=0.01\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# CIFAR10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Data loader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test,\n",
        "                                         shuffle=False)\n",
        "\n",
        "class_names = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "EoTMx7twuqXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1\n",
        "\n",
        "Crea una arquitectura de red neuronal MLP que procese datos con la forma Bx3x32x32, donde B representa el tamaño del batch, y que cuente con 10 neuronas en la capa de salida. Para ello, primero transforma el tensor de entrada de la forma Bx3x32x32 a Bx(33232) para que pueda ser utilizado en una capa lineal. Además, utiliza la función ReLU como activación en todas las capas ocultas y la función softmax en la capa de salida. Ten en cuenta que si aplicas nn.CrossEntropyLoss, no es necesario añadir ninguna función adicional en la última capa de la red.\n"
      ],
      "metadata": {
        "id": "IQVAosSwyrhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Texto de título predeterminado\n",
        "#Creamos la red neuronal\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "    # Define aquí las capas de tu red\n",
        "\n",
        "    self.fc1 = nn.Linear(3*32*32, 250) # 784 x 250 + 250 (bias: extra parameters)\n",
        "    self.fc2 = nn.Linear(250, 100) # 250 x 100 + 100\n",
        "    self.fc3 = nn.Linear(100, output_dim) # 100 x 10 + 10..\n",
        "\n",
        "  def forward(self, x):\n",
        "    #Implementa la función forward usando la función de activación ReLU en las capas ocultas\n",
        "    batch_size = x.shape[0]\n",
        "    #Flatten the input\n",
        "    input = x.view(batch_size, -1)\n",
        "\n",
        "    #Use ReLU as activation function\n",
        "    h_1 = F.relu(self.fc1(input))\n",
        "    h_2 = F.relu(self.fc2(h_1))\n",
        "    y_pred = self.fc3(h_2)\n",
        "\n",
        "    #Our network returns the output of the final layer but also the output of the hidden layer\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "\n",
        "    import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        # Transformación para capa lineal\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Capas ocultas\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 250)\n",
        "        self.fc2 = nn.Linear(250, 100)\n",
        "        self.fc3 = nn.Linear(100,num_classes)\n",
        "        # Función de activación: ReLU\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_flat = self.flatten(input)\n",
        "        # Uso de ReLU\n",
        "        h_1 = self.relu(self.fc1(input_flat))\n",
        "        h_2 = self.relu(self.fc2(h_1))\n",
        "        # Capa de salida (no aplicar softmax si se usa nn.CrossEntropyLoss)\n",
        "        y_pred = self.fc3(h_2)\n",
        "        return y_pred\n",
        "\n",
        "# Ejemplo de uso:\n",
        "model = MLP()\n"
      ],
      "metadata": {
        "id": "H-CbF71rykE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ejecutar el entrenamiento de tu primer modelo, ejecuta la siguiente celda y debería reportarte el accuracy de prueba de este primer experimento. Cuánto obtienes de accuracy de test?"
      ],
      "metadata": {
        "id": "uef4mTvDzjJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iniciar_semillas() # Se inicializan las semillas\n",
        "network = MLP() # Creas la red\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate) #Creas el optimizador\n",
        "train_complete(network, optimizer, train_loader, test_loader, 'mlp') #Entrenas la red. IMPORTANTE: el número de épocas siempre debe ser el mismo (internamente son 10 épocas)"
      ],
      "metadata": {
        "id": "3fv2eA_CzfRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d0b521-6734-409a-9dad-cebf7cfcc6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 794,360 trainable parameters\n",
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 2.186 | Train Acc: 25.42%\n",
            "\t Val. Loss: 1.806 |  Val. Acc: 35.22%\n",
            "Epoch: 02 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.742 | Train Acc: 37.35%\n",
            "\t Val. Loss: 1.663 |  Val. Acc: 39.62%\n",
            "Epoch: 03 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.602 | Train Acc: 42.65%\n",
            "\t Val. Loss: 1.577 |  Val. Acc: 43.50%\n",
            "Epoch: 04 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.510 | Train Acc: 46.27%\n",
            "\t Val. Loss: 1.514 |  Val. Acc: 46.60%\n",
            "Epoch: 05 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.432 | Train Acc: 49.21%\n",
            "\t Val. Loss: 1.492 |  Val. Acc: 47.43%\n",
            "Epoch: 06 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.374 | Train Acc: 51.54%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 48.75%\n",
            "Epoch: 07 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.318 | Train Acc: 53.41%\n",
            "\t Val. Loss: 1.439 |  Val. Acc: 49.00%\n",
            "Epoch: 08 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.271 | Train Acc: 55.03%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 49.57%\n",
            "Epoch: 09 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.236 | Train Acc: 56.52%\n",
            "\t Val. Loss: 1.432 |  Val. Acc: 50.63%\n",
            "Epoch: 10 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 1.194 | Train Acc: 57.95%\n",
            "\t Val. Loss: 1.463 |  Val. Acc: 50.16%\n",
            "Test Loss: 1.432 | Mejor test acc: 50.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2\n",
        "En esta parte vas a comparar el accuracy de test de la primera red que creaste (red multilayer perceptron) y los accuracys obtenidos usando redes convolucionales. Para completar esta parte debes hacer lo siguiente:\n",
        "\n",
        "\n",
        "\n",
        "*   Implementar la red convolucional: Crea una arquitectura que reciba imágenes RGB de 32x32 y tenga 10 neuronas de salida. Usa ReLU como función de activación , softmax en la capa de salida, y añade capas de max pooling para reducir el tamaño de los mapas de características.\n",
        "*   Entrena cada red usando la función \"train_complete\" usando el mismo optimizador siempre.\n",
        "*   Registra los accuracys que obtengas. Prueba distintas combinaciones de hiperparametros y arquitecturas para obtener los mejores resultados posibles. Evita utilizar redes muy profundas ya que estas pueden tener mayor dificultad para aprender.\n",
        "*   Discute los resultados obtenidos. Incluye en tu analisis la respuesta a las siguientes preguntas:\n",
        "    * Teóricamente, ¿qué tipo de red debería obtener mejores resultados al tratar con imágenes: una red convolucional o una MLP? ¿Por qué?\n",
        "    * Según tus resultados, ¿qué arquitectura funcionó mejor? ¿Por qué crees que esto ocurrió así?\n",
        "    * Antes mencionamos que las redes muy profundas son más difíciles de entrenar. ¿Al probar distintas arquitecturas, encontraste que se daba este fenómeno? ¿Por qué crees que ocurre esto? ¿Cómo podrías intentar solucionarlo?\n",
        "\n"
      ],
      "metadata": {
        "id": "fQ0blzDB0xB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos la red neuronal\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    # Define aquí las capas de tu red\n",
        "    # Capas convolucionales\n",
        "    ## Toma imágenes de 3 canales RGB y produce 16 mapas > 32 mapas > 64 mapas\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "\n",
        "    # Capas densas (fully-connected)\n",
        "    self.fc1 = nn.Linear(64*6*6, 256)\n",
        "    self.fc2 = nn.Linear(256,10)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2)) # de 3 × 32 × 32 → 16 × 16 × 16\n",
        "    x = F.relu(F.max_pool2d(self.conv2(x), 2)) # de 16 × 16 × 16 → 32 × 8 × 8\n",
        "    x = F.relu(self.conv3(x)) # de 32 × 8 × 8 → 64 × 6 × 6\n",
        "    x = x.view(-1, 64*6*6) # Aplana el vector\n",
        "    x = F.relu(self.fc1(x)) # de 1600 a 256\n",
        "    x = self.fc2(x) # de 256 a 10\n",
        "    return F.log_softmax(x) # función de densidad de probabilidades logaritmicas"
      ],
      "metadata": {
        "id": "b_JMsxPd0xJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iniciar_semillas() # Se inicializan las semillas\n",
        "network = CNN() # Creas la red\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate) #Creas el optimizador\n",
        "train_complete(network, optimizer, train_loader, test_loader, 'cnn') #Entrenas la red. IMPORTANTE: el número de épocas siempre debe ser el mismo (internamente son 10 épocas)"
      ],
      "metadata": {
        "id": "qEREXQXj2sVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fd80bc-f119-4a9a-fc73-60d7dd78c7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 616,234 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1268658930.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x) # función de densidad de probabilidades logaritmicas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 1.817 | Train Acc: 32.50%\n",
            "\t Val. Loss: 1.465 |  Val. Acc: 46.38%\n",
            "Epoch: 02 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 1.342 | Train Acc: 51.49%\n",
            "\t Val. Loss: 1.242 |  Val. Acc: 55.15%\n",
            "Epoch: 03 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 1.178 | Train Acc: 57.78%\n",
            "\t Val. Loss: 1.116 |  Val. Acc: 59.90%\n",
            "Epoch: 04 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 1.029 | Train Acc: 63.48%\n",
            "\t Val. Loss: 1.033 |  Val. Acc: 63.05%\n",
            "Epoch: 05 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.944 | Train Acc: 66.42%\n",
            "\t Val. Loss: 1.005 |  Val. Acc: 64.40%\n",
            "Epoch: 06 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.844 | Train Acc: 70.20%\n",
            "\t Val. Loss: 1.005 |  Val. Acc: 64.41%\n",
            "Epoch: 07 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.796 | Train Acc: 72.00%\n",
            "\t Val. Loss: 0.965 |  Val. Acc: 67.17%\n",
            "Epoch: 08 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.704 | Train Acc: 75.05%\n",
            "\t Val. Loss: 0.972 |  Val. Acc: 66.44%\n",
            "Epoch: 09 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.645 | Train Acc: 77.16%\n",
            "\t Val. Loss: 0.987 |  Val. Acc: 66.88%\n",
            "Epoch: 10 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.596 | Train Acc: 78.86%\n",
            "\t Val. Loss: 0.994 |  Val. Acc: 68.00%\n",
            "Test Loss: 0.994 | Mejor test acc: 68.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 3\n",
        "En esta parte, probaremos qué ocurre si se eliminan las capas de max pooling de la red que creaste en la parte 2. Para completar esta sección, debes hacer lo siguiente:\n",
        "\n",
        "*   Copia la implementación de la red convolucional y elimina las capas de pooling. Intenta cambiar lo menos posible la implementación, solo adapta las dimensiones de la red para ajustarlas tras haber removido el pooling.\n",
        "*   Entrena cada red usando la función \"train_complete\" usando el mismo optimizador siempre.\n",
        "*   Registra los accuracys que obtengas.\n",
        "*   Discute los resultados obtenidos. Incluye en tu análisis la respuesta a las siguientes preguntas:\n",
        "    * ¿Cómo se comparan los resultados obtenidos con los del modelo de la parte anterior? ¿Cómo explicas lo obtenido experimentalmente? ¿Cómo varía la cantidad de parámetros del modelo de la parte anterior en comparación con este?\n",
        "    * ¿Cuál es la utilidad de las capas de pooling? Si utilizáramos imágenes de mayor tamaño, ¿crees que el pooling sería más o menos necesario? Justifica tu respuesta."
      ],
      "metadata": {
        "id": "awnWUNIX-cgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos la red neuronal\n",
        "class CNN_sin_pooling(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_sin_pooling, self).__init__()\n",
        "    ## Toma imágenes de 3 canales RGB y produce 16 mapas > 32 mapas > 64 mapas\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "\n",
        "    # Capas densas (fully-connected)\n",
        "    self.fc1 = nn.Linear(64*30*30, 256)\n",
        "    self.fc2 = nn.Linear(256,10)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x)) # Removed the extra '2'\n",
        "    x = F.relu(self.conv2(x)) # de 16 × 32 × 32 → 32 × 32 × 32\n",
        "    x = F.relu(self.conv3(x)) # de 32 × 32 × 32 → 64 × 30 × 30\n",
        "    x = x.view(-1, 64*30*30) # Aplana el vector\n",
        "    x = F.relu(self.fc1(x)) # de 1600 a 256\n",
        "    x = self.fc2(x) # de 256 a 10\n",
        "    return F.log_softmax(x) # función de densidad de probabilidades logaritmicas# Define aquí las capas de tu red"
      ],
      "metadata": {
        "id": "GzKLonRU-26x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iniciar_semillas() # Se inicializan las semillas\n",
        "network = CNN_sin_pooling() # Creas la red\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate) #Creas el optimizador\n",
        "train_complete(network, optimizer, train_loader, test_loader, 'CNN_sin_pooling') #Entrenas la red. IMPORTANTE: el número de épocas siempre debe ser el mismo (internamente son 10 épocas)"
      ],
      "metadata": {
        "id": "o3rlRM5h_fCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d59572e-a75f-4179-9e13-ccd4e771981f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 14,772,010 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3169413019.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x) # función de densidad de probabilidades logaritmicas# Define aquí las capas de tu red\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 3.060 | Train Acc: 9.92%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 02 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 2.303 | Train Acc: 10.04%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 03 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 2.303 | Train Acc: 9.89%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 04 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 2.303 | Train Acc: 9.74%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 05 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 2.303 | Train Acc: 9.72%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 06 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 2.303 | Train Acc: 10.02%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 07 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 2.303 | Train Acc: 9.85%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 08 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 2.303 | Train Acc: 10.04%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 09 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 2.303 | Train Acc: 9.70%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Epoch: 10 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 2.303 | Train Acc: 9.82%\n",
            "\t Val. Loss: 2.303 |  Val. Acc: 10.00%\n",
            "Test Loss: 2.303 | Mejor test acc: 10.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 4\n",
        "\n",
        "En esta parte probaremos la arquitectura de ResNet y como este resuelve el problema del desvanecimiento de gradiente. Para eso ejecuta las siguientes celdas y responde las siguientes preguntas:\n",
        "\n",
        "* ¿Cómo se comparan los resultados con los de las partes anteriores? ¿Cómo varía la cantidad de parámetros?\n",
        "* ¿Cómo se estructuran las conexiones en ResNet para abordar el problema del desvanecimiento del gradiente y qué papel juegan las \"skip connections\" en esta arquitectura?\n"
      ],
      "metadata": {
        "id": "499STbGhVS1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet34"
      ],
      "metadata": {
        "id": "8LIQef3PWIVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = resnet34()\n",
        "m.fc = nn.Linear(512, 10)"
      ],
      "metadata": {
        "id": "RAdg9NnGWK1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iniciar_semillas() # Se inicializan las semillas\n",
        "network = m # Creas la red\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate) #Creas el optimizador\n",
        "train_complete(network, optimizer, train_loader, test_loader, 'resnet') #Entrenas la red. IMPORTANTE: el número de épocas siempre debe ser el mismo (internamente son 10 épocas)"
      ],
      "metadata": {
        "id": "c2B-7iiCWOYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rpAUBARGTlJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}